---
title: "DADA2 sample processing for *A. cervicornis* microplastics project"
date: "*Last run on `r format(Sys.time(), '%d %B %Y')`*"
output: 
  html_document:
  theme: flatly
toc: yes
toc_depth: 3
toc_float: yes
---

```{r set some defaults, echo = FALSE}

library("knitr")
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
pdf.options(useDingbats = FALSE)

```

```{r install dada2, eval = FALSE, include = FALSE}

## Downloading DADA2 following instructions found here:
# https://benjjneb.github.io/dada2/dada-installation.html

# these instructions for the SCC and MacBook Pro (2020)
install.packages("devtools")
library("devtools")
devtools::install_github("benjjneb/dada2", ref="v1.20", force = TRUE) # change the ref argument to get other versions


## install phyloseq packages
install.packages("Biostrings")

if (!require("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
BiocManager::install(version = "3.14")
BiocManager::install("phyloseq")

## used for removing contamination from neg controls
BiocManager::install("decontam")

```

```{r load packages, echo = FALSE}

library(dada2) # I have version 1.18.0 downloaded on the SCC for this
library(tidyverse)
library(ShortRead)
library(kableExtra)
library(phyloseq)
library(decontam)
library(RColorBrewer)
library(plotly)
library(vegan)

```

```{r setup standards}

## Set the treatment colours:
trt_col <- c("dodgerblue", "darkorange","firebrick2", "firebrick4")

```


<br/>

I am largely following the [DADA2](https://www.bioconductor.org/packages/release/bioc/manuals/dada2/man/dada2.pdf) tutorial by Benjamin Callahan that can be found [here](https://benjjneb.github.io/dada2/tutorial.html). 


## Inspecting the data {.tabset}

```{r set path to 16S samples}

## the directory containing the fastq files after unzipping (ifelse will set  path specific to working on SCC vs locally)
wd <- getwd()

if(length(grep("Dropbox", wd)) > 0) {
  path_16S <- "/Users/colleen/Dropbox/BU/DaviesLab/ACER_microplastics/16S_fastq" 
} else {
  path_16S <- "/projectnb/davieslab/bove/acer_MP/final_samples/16S_29Jun22" 
}

# looks good! need to specify only using the microplastic samples
# list.files(path_16S)

```

```{r extract fastq files and sample names}

# Forward and reverse fastq filenames have format: SAMPLENAME_R1.fastq and SAMPLENAME_R2.fastq
fnFs <- sort(list.files(path_16S, pattern = "_R1.fastq", full.names = TRUE))
fnRs <- sort(list.files(path_16S, pattern = "_R2.fastq", full.names = TRUE))

# Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq
sample_names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)

```

### Check samples to use

```{r select only microplastic samples}

## read in the meta data from the samples
meta <- read.csv("Data/Acerv_MP_Metadata.csv")

# filter the sample name list to select neg controls and microplastic samples only
sample_names <- sample_names[sample_names %in% meta$sampleID]
sample_names

# reordering these so they match the order of sample names 
meta <- meta[match(sample_names, meta$sampleID),] 
rownames(meta) <- meta$sampleID


# filter the forward sequences for microplastics only
fnFs <- fnFs %>% 
  str_detect("/A|/B|/C|G5|G6", negate = FALSE) %>%
  keep(fnFs, .)

# filter the reverse sequences for microplastics only
fnRs <- fnRs %>% 
  str_detect("/A|/B|/C|G5|G6", negate = FALSE) %>%
  keep(fnRs, .)

```


```{r set some standards for checking primers, include=FALSE}

#### check for primers ####
FWD <- "GTGYCAGCMGCCGCGGTAA"  ## CHANGE ME to your forward primer sequence
REV <- "GGACTACNVGGGTWTCTAAT"  ## CHANGE ME...

allOrients <- function(primer) {
  # Create all orientations of the input sequence
  require(Biostrings)
  dna <- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors
  orients <- c(Forward = dna, Complement = complement(dna), Reverse = reverse(dna), 
               RevComp = reverseComplement(dna))
  return(sapply(orients, toString))  # Convert back to character vector
}

FWD.orients <- allOrients(FWD)
REV.orients <- allOrients(REV)


primerHits <- function(primer, fn) {
  # Counts number of reads in which the primer is found
  nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
  return(sum(nhits > 0))
}

```

```{r double check for primers left, eval=FALSE, include=FALSE}

fnFs.filtN <- file.path(path_16S, "filtN", basename(fnFs)) # Put N-filterd files in filtN/ subdirectory
fnRs.filtN <- file.path(path_16S, "filtN", basename(fnRs))
filterAndTrim(fnFs, fnFs.filtN, fnRs, fnRs.filtN, maxN = 0, multithread = TRUE)


#towards the beginning
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.filtN[[1]]), 
      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.filtN[[1]]), 
      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.filtN[[1]]), 
      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.filtN[[1]]))

#at the end
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.filtN[[4]]), 
      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.filtN[[4]]), 
      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.filtN[[4]]), 
      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.filtN[[4]]))

```

```{r remove the extra primers that were left, eval=FALSE, include=FALSE}

fnFs_cut <- file.path(path_16S, "cutadapt", basename(fnFs)) # 16S path with the subdirectory 'cutadapt' then the sample names
fnRs_cut <- file.path(path_16S, "cutadapt", basename(fnRs))

#towards the beginning
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs_cut[[1]]), 
      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs_cut[[1]]), 
      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs_cut[[1]]), 
      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs_cut[[1]]))

#at the end
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs_cut[[4]]), 
      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs_cut[[4]]), 
      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs_cut[[4]]), 
      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs_cut[[4]]))


# no more primers were detected after this second cutadapt step

```

<br/>
<br/>


### View example quality plots

Viewing the quality of 2 of the forward samples
```{r firward quality plots, fig.height = 4, fig.width = 6}

fnFs_cut <- file.path(path_16S, "cutadapt", basename(fnFs)) # 16S path with the subdirectory 'cutadapt' then the sample names
fnRs_cut <- file.path(path_16S, "cutadapt", basename(fnRs))


## Plot 2 samples for examples:
plotQualityProfile(fnFs_cut[1:2])

```

Viewing the quality of the same 2 of the reverse samples
```{r reverse quality plots, fig.height = 4, fig.width = 6}

## Plot 2 reverse samples for examples:
plotQualityProfile(fnRs_cut[1:2])

```

```{r eval=FALSE, include=FALSE}

## Saving the quality profiles of all (except sample A1) to view later if necessary
Fs_qualityPlot <- plotQualityProfile(fnFs_cut)

png(file = "Figures/Forward_qualityPlots.png", width = 30, height = 23, res = 1200, unit = "cm")
Fs_qualityPlot
dev.off()


## Saving the quality profiles of all to view later if necessary
Rs_qualityPlot <- plotQualityProfile(fnRs_cut)

png(file = "Figures/Reverse_qualityPlots.png", width = 30, height = 23, res = 1200, unit = "cm")
Rs_qualityPlot
dev.off()

```

<br/>
<br/>


## Filter and trim samples {.tabset}

### Perform the filtering

You can see in above figures that the quality of reads drop off towards the end, so we need to filter out these low quality reads

```{r filtering low quality reads}

# Place filtered files in filtered/ subdirectory
filtFs <- file.path(path_16S, "filtered", paste0(sample_names, "_filt_F.fastq.gz"))
filtRs <- file.path(path_16S, "filtered", paste0(sample_names, "_filt_R.fastq.gz"))

# name the file paths with the corresponding sample names
names(filtFs) <- sample_names
names(filtRs) <- sample_names


filt_out <- filterAndTrim(fnFs_cut, filtFs, fnRs_cut, filtRs,
                     truncLen = c(220, 210), # cut for the forward and reverse files
                     maxN = 0, # DADA2 requires no Ns
                     maxEE = c(1, 1), # reads with higher than maxEE "expected errors" will be discarded
                     truncQ = 2, # Truncate reads at the first instance of a quality score less than or equal to truncQ
                     rm.phix = TRUE, # If TRUE, discard reads that match against the phiX genome (kinda control bacteria)
                     compress = TRUE, #  Whether the output fastq file should be gzip com- pressed.
                     multithread = TRUE) # On Windows set multithread = FALSE
head(filt_out)

# the percent of reads making it through the filter and trim step:
filter_perc <- (colSums(filt_out)[2]/colSums(filt_out)[1] * 100) # 95.5%

```

After filtering out the low quality reads, we have maintained about `r round(filter_perc, 1)` of the original reads.

<br/>
<br/>

### Check the filtering

Viewing the quality of the same 2 of the forward samples post filtering and trimming
```{r forward quality plots post filter trim, fig.height = 3, fig.width = 6, fig.align = "center"}

## plot the filtered forward samples
plotQualityProfile(filtFs[1:2])

```

```{r save forward quality plots post filter trim, eval=FALSE, include=FALSE}

## save the filtered forward quality plots
png(file = "Figures/Forward_filt_qualityPlots.png", width = 30, height = 23, res = 1200, unit = "cm")
plotQualityProfile(filtFs)
dev.off()

```

Viewing the quality of the same 2 of the reverse samples post filtering and trimming
```{r reverse quality plots post filter trim, fig.height = 3, fig.width = 6, fig.align = "center"}

## plot the filtered reverse samples
plotQualityProfile(filtRs[1:2])

```

```{r save reverse quality plots post filter trim, eval=FALSE, include=FALSE}

## save the filtered reverse quality plots
png(file = "Figures/Reverse_filt_qualityPlots.png", width = 30, height = 23, res = 1200, unit = "cm")
plotQualityProfile(filtRs)
dev.off()

```

<br/>
<br/>


## DADA2 sample processing {.tabset}

### Error Rates

```{r learn errors and save, eval=FALSE, include=FALSE}

### Learning the error rates takes a few minutes so I am saving it as an R object to call in line. If things are modified upstream, this needs to be rerun.

errF <- learnErrors(filtFs, multithread = TRUE)
errR <- learnErrors(filtRs, multithread = TRUE)

# visualize errors
errF_plot <- plotErrors(errF, nominalQ = TRUE)
errR_plot <- plotErrors(errR, nominalQ = TRUE)


## Save the error rates
save(errF, errR, errF_plot, errR_plot, file = "Data/dada_phylo/dada_errors.Rdata")

```

```{r load and viz errors, fig.height = 5, fig.width = 7, fig.align = "center"}

load("Data/dada_phylo/dada_errors.Rdata")

errF_plot

```

The error rates for each possible transition (A→C, A→G, …) are shown. Points are the observed error rates for each consensus quality score. The black line shows the estimated error rates after convergence of the machine-learning algorithm. The red line shows the error rates expected under the nominal definition of the Q-score. We want the estimated error rates (black line) to be a good fit to the observed rates (points), and the error rates to drop with increased quality.

<br/>
<br/>


### Sample Inference

We are now ready to apply [the core sample inference algorithm](https://www.nature.com/articles/nmeth.3869#methods) to the filtered and trimmed sequence data.

<br/>

**Forward sample inference**
```{r forward sample inference}

# Core sample inference of forward samples
dadaFs <- dada(filtFs, err = errF, multithread = TRUE)

```

Inspecting the returned dada-class object for the first forward sample:
```{r view dada class object}

# Inspecting the returned dada-class object:
dadaFs[[1]]

```

The DADA2 algorithm inferred 144 true sequence variants from the 9515 unique sequences in the first sample. There is much more to the dada-class return object than this (see help(`"dada-class"`) for some info), including multiple diagnostics about the quality of each denoised sequence variant, but that is beyond the scope of an introductory tutorial.

<br/>

**Reverse sample inference**
```{r reverse sample inference}

# Core sample inference of reverse samples
dadaRs <- dada(filtRs, err = errR, multithread = TRUE)

```

<br/>
<br/>


### Merge paired reads

We now merge the forward and reverse reads together to obtain the full denoised sequences. Merging is performed by aligning the denoised forward reads with the reverse-complement of the corresponding denoised reverse reads, and then constructing the merged “contig” sequences. By default, merged sequences are only output if the forward and reverse reads overlap by at least 12 bases, and are identical to each other in the overlap region (but these conditions can be changed via function arguments).

```{r merge paired reads}

mergers <- mergePairs(dadaF = dadaFs, # dada-class object(s) generated by denoising the forward reads.
                      derepF = filtFs, # derep-class object(s) used as input to the the dada function when denoising (for)
                      dadaR = dadaRs, # dada-class object(s) generated by denoising the reverse reads.
                      derepR = filtRs, # derep-class object(s) used as input to the the dada function when denoising (rev)
                      verbose = TRUE) # summary of the function results are printed to standard output (verbose = TRUE)

# Inspect the merger data.frame from the first sample
#head(mergers[[1]])

```

The mergers object is a list of data.frames from each sample. Each data.frame contains the merged sequence, its abundance, and the indices of the forward and reverse sequence variants that were merged. Paired reads that did not exactly overlap were removed by mergePairs, further reducing spurious output.

<br/>
<br/>


### Construct sequence table

We can now construct an amplicon sequence variant table (ASV) table, a higher-resolution version of the OTU table produced by traditional methods.

<br/>

```{r make ASV table}

seqtab <- makeSequenceTable(mergers)
#dim(seqtab) # 38 samples, 3546 sequence variants

# Inspect distribution of read lengths
table(nchar(getSequences(seqtab)))
hist(nchar(getSequences(seqtab)))

```

The sequence table is a matrix with rows corresponding to (and named by) the samples, and columns corresponding to (and named by) the sequence variants. This table contains `r dim(seqtab)[2]` ASVs.

<br/>

After viewing the distribution of read lengths, it looks like we have some that fall outside the expected range (244 - 264) so we will go ahead and remove these non target length sequences.

```{r remove nontarget sequences}

# Remove any ASVs that are considerably off target length
seqtab_trimmed <- seqtab[,nchar(colnames(seqtab)) %in% seq(244,264)]
table(nchar(getSequences(seqtab_trimmed)))

```

This updated table now contains `r dim(seqtab_trimmed)[2]` ASVs.

<br/>
<br/>


### Remove chimeras

The core dada method corrects substitution and indel errors, but chimeras remain. Fortunately, the accuracy of sequence variants after denoising makes identifying chimeric ASVs simpler than when dealing with fuzzy OTUs. Chimeric sequences are identified if they can be exactly reconstructed by combining a left-segment and a right-segment from two more abundant “parent” sequences.

```{r find chimeras}

seqtab_nochim <- removeBimeraDenovo(seqtab_trimmed, method = "consensus", multithread = TRUE, verbose = TRUE)
#dim(seqtab_nochim)
rownames(seqtab_nochim) <- gsub("_filt_F.fastq.gz", "", rownames(seqtab_nochim))

perc_nochim <- (sum(seqtab_nochim)/sum(seqtab) * 100) # percent non chimera sequences kept


## Save the chimera-free ASV table to load in downstream analyses
save(seqtab_nochim, file = "Data/dada_phylo/seqtab_nochim.Rdata")

```

A total of `r dim(seqtab_trimmed)[2] - dim(seqtab_nochim)[2]` bimeras were identified from the `r dim(seqtab_nochim)[2]` input sequences, thus retaining `r round(perc_nochim, 1)`% of sequences.

<br/>
<br/>


### Track reads through the pipeline

As a final check of our progress, we can look at the number of reads that made it through each step in the pipeline:

```{r track reads through pipeline}

getN <- function(x) sum(getUniques(x)) # write function to count reads for all samples through time

track_reads <- cbind(meta[c(1,3:4)], filt_out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab_nochim))
# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)

colnames(track_reads) <- c("Sample ID", "Genotype", "Treatment", "Input", "Filtered", "Denoised Forward", "Denoised Reverse", "Merged", "Nonchimera")
rownames(track_reads) <- sample_names

```

```{r kable of reads through pipeline}

kable(track_reads[-1], row.names = FALSE) %>% 
  #kable_minimal() %>% 
  kable_classic_2() %>% 
  kable_styling(font_size = 14, full_width = FALSE) %>% 
  column_spec(1, bold = TRUE) %>% 
  column_spec(2, border_right = TRUE)

write.csv(track_reads, "Tables/Track_Reads.csv", row.names = FALSE)

```

<br/>
<br/>


## Assign taxonomy

It is common at this point, especially in 16S/18S/ITS amplicon sequencing, to assign taxonomy to the sequence variants. The DADA2 package provides a native implementation of the naive Bayesian classifier method for this purpose. The `assignTaxonomy` function takes as input a set of sequences to be classified and a training set of reference sequences with known taxonomy, and outputs taxonomic assignments with at least `minBoot` bootstrap confidence.

The dada2 package GitHub maintains the most updated versions of the [Silva databases.](https://benjjneb.github.io/dada2/training.html), but I downloaded the databases from the associated [Zenodo](https://zenodo.org/record/4587955#.YuBdCS-cbOQ). The versions in this GitHub repository, used here, were last updated on 26 July 2022.

```{r SILVA path, eval=FALSE, include=FALSE}

## going to move these out of this repo because they are too big:
# SCC path to SILVA files: /projectnb/davieslab/bove/SILVA_files
# local path to SILVA files: /Users/colleen/Dropbox/BU/DaviesLab/ACER_microplastics/SILVA/

if(length(grep("Dropbox", wd)) > 0) {
  silva_path <- "/Users/colleen/Dropbox/BU/DaviesLab/ACER_microplastics/SILVA/silva_nr99_v138.1_train_set.fa.gz" 
} else {
  silva_path <- "/projectnb/davieslab/bove/SILVA_files/silva_nr99_v138.1_train_set.fa.gz" 
}

taxa <- assignTaxonomy(seqtab_nochim, silva_path, multithread=TRUE)



# Extensions: The dada2 package also implements a method to make species level assignments based on exact matching between
# ASVs and sequenced reference strains. Recent analysis suggests that exact matching (or 100% identity) is the only
# appropriate way to assign species to 16S gene fragments. Currently, species-assignment training fastas are available for
# the Silva and RDP 16S databases. To follow the optional species addition step, download the
# silva_species_assignment_v138.1.fa.gz file, and place it in the directory with the fastq files.

if(length(grep("Dropbox", wd)) > 0) {
  silva_sp_path <- "/Users/colleen/Dropbox/BU/DaviesLab/ACER_microplastics/SILVA/silva_species_assignment_v138.1.fa.gz" 
} else {
  silva_sp_path <- "/projectnb/davieslab/bove/SILVA_files/silva_species_assignment_v138.1.fa.gz" 
}

taxa_sp <- addSpecies(taxa, silva_sp_path)



## Save objects to save time running (but will need to rerun if making upstream adjustments)
save(taxa, taxa_sp, file = "Data/dada_phylo/taxa_assign.Rdata")

```

Let’s inspect the taxonomic assignments:

```{r load phylo taxa objects and view}

load("Data/dada_phylo/taxa_assign.Rdata")

taxa_print <- taxa_sp # Removing sequence rownames for display only
rownames(taxa_print) <- NULL
head(taxa_print)

```

```{r create ASV table using phyloseq}

# Export sequence table with genus and species assignments as phyloseq objects
phylo <- phyloseq(otu_table(seqtab_nochim, taxa_are_rows = FALSE), sample_data(meta), tax_table(taxa))
phylo_sp <- phyloseq(otu_table(seqtab_nochim, taxa_are_rows = FALSE), sample_data(meta), tax_table(taxa_sp))

# originally, sequences were the taxa names
seqs_ps <- taxa_names(phylo_sp)

# change taxa names to ASVs
taxa_names(phylo_sp) <- paste0("ASV", seq(ntaxa(phylo_sp)))

# save a file of the taxa table with ASVs and sequences
taxtab_seqs <- cbind(data.frame(tax_table(phylo_sp)), seqs_ps)
write.csv(taxtab_seqs,"Data/dada_phylo/taxatable_withsequences.csv")


# Save as RDS objects
save(phylo, phylo_sp, file = "Data/dada_phylo/phylo_objs.Rdata") # save both with and without species level and ASV labeling

```

Great! We can now save this and hand it off to Phyloseq for further analyses.

<br/>
<br/>


## Remove contaminations {.tabset}

### Remove mitochondria, chloroplasts, and non-bacteria

```{r load phylo objects}

## Load the phyloseq objects created above
load(file = "Data/dada_phylo/phylo_objs.Rdata")

```

```{r remove contam taxa}

# remove all mitochondria from family  
ps_nomito <- subset_taxa(phylo_sp, (Family != "Mitochondria") | is.na(Family))

# remove all chloroplast from order
ps_nochlor <- subset_taxa(ps_nomito, (Order != "Chloroplast") | is.na(Order))

# remove all non-bacteria
ps_clean <- subset_taxa(ps_nochlor, (Kingdom == "Bacteria"))
ps_clean

```

Removal of mitochondira, chloroplasts, and non-bacteria taxa reduced the total number of taxa from `r dim(phylo_sp@otu_table)[[2]]` to `r dim(ps_clean@otu_table)[[2]]`. 

<br/>
<br/>


### Remove neg control contamination

```{r library size, fig.height = 4, fig.width = 6, fig.align = "center"}

## Visualize library sizes per sample
df <- data.frame(sample_data(ps_clean)) # make sample_data a dataframe
df$LibrarySize <- sample_sums(ps_clean) # add reads per sample
df <- df[order(df$LibrarySize),] # reorder df from lowest to highest library size
df$Index <- seq(nrow(df)) # add index for plotting

# plot library size by treatment
ggplot(data = df, aes(x = Index, y = LibrarySize, color = Treatment)) + 
  geom_point() +
  ggtitle("Library size by treatment of all samples")

## identify contaminants based on neg controls
sample_data(ps_clean)$is.neg <- sample_data(ps_clean)$Treatment == "blank_control" # identify neg controls & add to sample data
contamdf_freq <- isContaminant(ps_clean, neg = "is.neg", threshold = 0.5) # The frequency of each sequence (or OTU) in the input feature table as a function of the concentration of amplified DNA in each sample is used to identify contaminant sequences.

neg_contam <- table(contamdf_freq$contaminant)
#head(which(contamdf_freq$contaminant)) # which ASVs are being identified as contaminants 

```

Just `r neg_contam[[2]]` out of the `r nrow(contamdf_freq)` ASVs were classified as contaminants. 

<br/>

```{r neg control contam removal, fig.height = 4, fig.width = 6, fig.align = "center"}

# Make phyloseq object of presence-absence in negative controls and true samples
ps_pa <- transform_sample_counts(ps_clean, function(abund) 1 * (abund > 0)) # transforms the sample counts of a taxa abundance matrix
ps_pa_neg <- prune_samples(sample_data(ps_pa)$Treatment == "blank_control", ps_pa) # filters samples for controls
ps_pa_pos <- prune_samples(sample_data(ps_pa)$Treatment != "blank_control", ps_pa) # filters to remove controls

# Make dataframe of prevalence in positive and negative samples
df_pa <- data.frame(pa_pos = taxa_sums(ps_pa_pos),
                    pa_neg = taxa_sums(ps_pa_neg),
                    contaminant = contamdf_freq$contaminant)

# plot prevalence of neg controls against true samples
ggplot(data = df_pa, aes(x = pa_neg, y = pa_pos, color = contaminant)) +
  geom_point() +
  xlab("Prevalence (Negative Controls)") +
  ylab("Prevalence (True Samples)")

# remove contam taxa from ps_clean:
ps_clean1 <- prune_taxa(!contamdf_freq$contaminant, ps_clean)

# also remove negative controls, don't need them anymore I think
ps_cleaner <- subset_samples(ps_clean1, (Treatment != "blank_control"))

ps_cleaner # 3272 taxa left, 36 samples

```

Looks good! We have now cleaned up the sample data to remove contamination from non target organisms and those from the negative controls.

<br/>
<br/>


## SECTION TITLE {.tabset}

### Blast NCBI

```{r save the sequences to fasta for blast, eval=FALSE, include=FALSE}

## make output fasta file 
ids <- paste0("sq", seq(1, length(colnames(seqtab_nochim))))
uniquesToFasta(seqtab_nochim, "Data/acer_cleaned_16s.fasta", ids = ids, mode = "w", width = 20000)

```

```{r eval=FALSE, include=FALSE}

######## Blast ASVs against NCBI
### This portion was run on the SCC on XXXX and the script can also be found on 'acer_SCC_pipeline_28Jun22.sh' script

```


### Rarefy

```{r}

rarecurve(seqtable_no0, step = 100, label = FALSE) # after removing contaminants

total <- rowSums(seqtable_no0)
toofew <- subset(total, total <9100)
# no samples to remove here

# do not need to do this since we don't need to remove any low samples
#row2remove <- c(names(toofew))
#seqtab_less <- seqtable_no0[!(row.names(seqtable_no0) %in% row2remove),]
#samdf_rare <- samdf[!(row.names(samdf) %in% row2remove), ]

seqtab_rare <- rrarefy(seqtable_no0, sample=9100)


save(seqtab_rare, file = "Data/dada_phylo/seqtab_rare9100.Rdata")


rare_zero <- seqtab_rare[,colSums(seqtab_rare) == 0]
ncol(rare_zero) # 84
removecols <- c(colnames(rare_zero))
seqtab_rare_no0 <- seqtab_rare[,!(colnames(seqtab_rare) %in% removecols)]

samdf_rare$year <- as.factor(samdf_rare$year)
samdf_rare$year <- gsub("16","15",samdf_rare$year)
samdf_rare$year <- as.factor(samdf_rare$year)
samdf_rare_nobis <- subset(samdf_rare, transect!="Biscayne")
ggplot(data.frame(samdf_rare_nobis), aes(x=year)) +
  geom_bar()

#phyloseq object but rarefied
ps.sid.rare <- phyloseq(otu_table(seqtab_rare.no0, taxa_are_rows=FALSE), 
                    sample_data(samdf.sid.rare.nobis), 
                    tax_table(taxa2))
ps.sid.rare #24526 taxa after removing 0s, 153 samples after rarefying

#### alpha diversity ####
#Visualize alpha-diversity - ***Should be done on raw, untrimmed dataset***
#total species diversity in a landscape (gamma diversity) is determined by two different things, the mean species diversity in sites or habitats at a more local scale (alpha diversity) and the differentiation among those habitats (beta diversity)
#Shannon:Shannon entropy quantifies the uncertainty (entropy or degree of surprise) associated with correctly predicting which letter will be the next in a diverse string. Based on the weighted geometric mean of the proportional abundances of the types, and equals the logarithm of true diversity. When all types in the dataset of interest are equally common, the Shannon index hence takes the value ln(actual # of types). The more unequal the abundances of the types, the smaller the corresponding Shannon entropy. If practically all abundance is concentrated to one type, and the other types are very rare (even if there are many of them), Shannon entropy approaches zero. When there is only one type in the dataset, Shannon entropy exactly equals zero (there is no uncertainty in predicting the type of the next randomly chosen entity).
#Simpson:equals the probability that two entities taken at random from the dataset of interest represent the same type. equal to the weighted arithmetic mean of the proportional abundances pi of the types of interest, with the proportional abundances themselves being used as the weights. Since mean proportional abundance of the types increases with decreasing number of types and increasing abundance of the most abundant type, λ obtains small values in datasets of high diversity and large values in datasets of low diversity. This is counterintuitive behavior for a diversity index, so often such transformations of λ that increase with increasing diversity have been used instead. The most popular of such indices have been the inverse Simpson index (1/λ) and the Gini–Simpson index (1 − λ).
plot_richness(ps.sid, x="transect_zone_year", measures=c("Shannon", "Simpson","Observed"), color="zone") + theme_bw()

df <- data.frame(estimate_richness(ps.sid, split=TRUE, measures=c("Shannon","InvSimpson","Observed")))
df
df <- data.frame(estimate_richness(ps.sid.rare, split=TRUE, measures=c("Shannon","InvSimpson","Observed")))

df$sample_16s <- rownames(df)
df$sample_16s <- gsub("X","",df$sample_16s)
df.div <- merge(df,samdf.sid.rare,by="sample_16s") #add sample data

write.csv(df,file="fl16s_diversity_sids.csv") #saving
df.div <- read.csv("fl16s_diversity_sids.csv") #reading back in 

#concatenating 15 with 16
df.div$year <- gsub("16","15",df.div$year)
df.div$year <- as.factor(df.div$year)
df.div.nobis <- subset(df.div,transect!="Biscayne")

quartz()
#zone
gg.sh <- ggplot(df.div.nobis, aes(x=zone, y=Shannon,color=zone,shape=zone))+
  geom_boxplot(outlier.shape=NA)+
  xlab("Reef zone")+
  ylab("Shannon diversity")+
  theme_cowplot()+
  facet_wrap(~transect*year)
gg.sh
  # scale_shape_manual(values=c(16,15),labels=c("Back reef","Fore reef"))+
  # scale_colour_manual(values=c("#ED7953FF","#8405A7FF"),labels=c("Back reef","Fore reef"))+
  # #guides(color=guide_legend(title="Reef zone"),shape=guide_legend(title="Reef zone"))+
  # theme(text=element_text(family="Times"),legend.position="none")+
  # geom_jitter(alpha=0.5)+

gg.sh <- ggplot(df.div.nobis, aes(x=year,y=Shannon,color=year))+
  geom_boxplot()+
  xlab("Year")+
  ylab("Shannon diversity")+
  theme_cowplot()+
  #facet_wrap(~transect)+
  scale_color_manual(values=c("#781C6D","#F8850F","#AE305C"),name="Time point",labels=c("Baseline","Irma","Post-Irma"))+
  scale_x_discrete(labels=c("2015","2017","2018"))+
  theme(axis.text.x=element_text(angle=45,hjust=1))
quartz()
gg.sh

gg.si <- ggplot(df.div.nobis, aes(x=transect, y=InvSimpson,color=year,shape=year))+
  geom_boxplot()+
  xlab("Reef zone")+
  ylab("Inv. Simpson diversity")+
  theme_cowplot()+
  ylim(0,100)#+
  #scale_shape_manual(values=c(16,15),labels=c("Back reef","Fore reef"))+
  #scale_colour_manual(values=c("#ED7953FF","#8405A7FF"),labels=c("Back reef","Fore reef"))+
  #guides(color=guide_legend(title="Reef zone"),shape=guide_legend(title="Reef zone"))+
  #theme(text=element_text(family="Times"),legend.position="none")+
  #geom_jitter(alpha=0.5)#+
  #facet_wrap(~site)
gg.si

gg.obs <- ggplot(df.div, aes(x=zone, y=Observed,color=zone,shape=zone))+
  geom_boxplot(outlier.shape=NA)+
  xlab("Reef zone")+
  ylab("Shannon diversity")+
  theme_cowplot()+
  scale_shape_manual(values=c(16,15),labels=c("Back reef","Fore reef"))+
  scale_colour_manual(values=c("#ED7953FF","#8405A7FF"),labels=c("Back reef","Fore reef"))+
  #guides(color=guide_legend(title="Reef zone"),shape=guide_legend(title="Reef zone"))+
  theme(text=element_text(family="Times"),legend.position="none")+
  geom_jitter(alpha=0.5)+
  facet_wrap(~site)
gg.obs

```


<br/>
<br/>


## Data visualization {.tabset}

```{r remove samples to match GE samples}

## Before proceeding, we are going to remove the samples that did not successfully prep for gene expression, were identified as clones, and those with low reads for the gene expression analysis to have matching samples. 


# list of samples to keep (removed failed RNA preps, low counts, and clones)
keep_samples <- c("10aA - 1", "5aC - 2", "10aB - 1", "6D - 1", "5aC - 1", "6B - 2", "5B - 1", "5aE - 1", "2E - 1", "2B - 1", "6B - 1", "10aC - 1", "5aA - 1", "5D - 1", "5C - 1", "10aD - 1", "5C - 2", "2D - 1", "6E - 1")


# ps_final <- subset_samples(ps_cleaner, (Genotype %in% keep_samples))
ps_final <- ps_cleaner

```

<br/>


### Preliminary figures

Taking phyloseq data and making some preliminary visualizations based on DADA2 tutorial:

```{r pulling top 20 ASVs}

## Modifying the function 
plot_bar <- function(physeq, x = "Sample", y = "Abundance", fill = NULL, colour = colour, title = NULL, facet_grid = NULL) {
  mdf = psmelt(physeq)
  p = ggplot(mdf, aes_string(x = x, y = y, fill = fill, colour = fill))
  p = p + geom_bar(stat = "identity", position = "stack")
  p = p + theme(axis.text.x = element_text(angle = -90, hjust = 0))
  if (!is.null(facet_grid)) {
    p <- p + facet_grid(facet_grid)
  }
  if (!is.null(title)) {
    p <- p + ggtitle(title)
  }
  return(p)
}

## pull top 20 ASVs 
top20 <- names(sort(taxa_sums(ps_final), decreasing=TRUE))[1:20]
ps_top20 <- transform_sample_counts(ps_final, function(OTU) OTU/sum(OTU))
ps_top20 <- prune_taxa(top20, ps_top20)

```

```{r plot the samples by treatment at Fmaily level, fig.width = 8, fig.height = 6, fig.align = "center"}

## plot top 20 ASVs by family
barplot <- plot_bar(ps_top20, x = "Genotype", fill = "Family", colour = "Family") + 
  facet_wrap(~ Treatment, scales = "free_x") +
  theme_bw() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  scale_fill_manual(values = colorRampPalette(brewer.pal(8, "Set2"))(15)) +
  scale_colour_manual(values = colorRampPalette(brewer.pal(8, "Set2"))(15)) +
  ggtitle("Top 20 ASVs at Family level")

ggplotly(barplot)

```

```{r plot the samples by treatment at Order level, fig.width = 8, fig.height = 6, fig.align = "center"}

## plot top 20 ASVs by order
barplot_order <- plot_bar(ps_top20, x = "Genotype", fill = "Order", colour = "Order") + 
  facet_wrap(~ Treatment, scales = "free_x") +
  theme_bw() +
  #theme(legend.position = "bottom") +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  scale_fill_manual(values = colorRampPalette(brewer.pal(8, "Set2"))(12)) +
  scale_colour_manual(values = colorRampPalette(brewer.pal(8, "Set2"))(12)) +
  ggtitle("Top 20 ASVs at Order level")

ggplotly(barplot_order)

```

```{r plot the samples by treatment at genus level, fig.width = 8, fig.height = 6, fig.align = "center"}

## plot top 20 ASVs by order
barplot_genus <- plot_bar(ps_top20, x = "Genotype", fill = "Genus", colour = "Genus") + 
  facet_wrap(~ Treatment, scales = "free_x") +
  theme_bw() +
  #theme(legend.position = "bottom") +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  scale_fill_manual(values = colorRampPalette(brewer.pal(8, "Set2"))(15)) +
  scale_colour_manual(values = colorRampPalette(brewer.pal(8, "Set2"))(15)) +
  ggtitle("Top 20 ASVs at Genus level")

ggplotly(barplot_genus)

```

<br/>
<br/>


### Principal component analyses

#### All microbiome

```{r}

```

<br/>


#### Core microbiome

```{r}

```

<br/>


#### Accessory microbiome

```{r}

```

<br/>
<br/>


### Shannon diversity

```{r}

```

<br/>
<br/>


### Simpson diversity

```{r}

```

<br/>
<br/>


## Session Information

All code was written by [Colleen B. Bove](https://colleenbove.science), feel free to contact with questions.

Session information from the last run date on `r format(Sys.time(), '%d %B %Y')`:

```{r print session info}

sessionInfo()

```
