---
title: "DADA2 sample processing for *A. cervicornis* microplastics project"
date: "*Last run on `r format(Sys.time(), '%d %B %Y')`*"
output: 
  html_document:
  theme: flatly
toc: yes
toc_depth: 3
toc_float: yes
---

```{r set some defaults, echo = FALSE}

library("knitr")
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
pdf.options(useDingbats = FALSE)

```

```{r install dada2, eval = FALSE, include = FALSE}

## Downloading DADA2 following instructions found here:
# https://benjjneb.github.io/dada2/dada-installation.html

install.packages("devtools")
library("devtools")
devtools::install_github("benjjneb/dada2", ref="v1.18") # change the ref argument to get other versions

```

```{r load packages, echo = FALSE}

library(dada2) # I have version 1.18.0 downloaded on the SCC for this
library(tidyverse)

```

<br/>

I am largely following the [DADA2](https://www.bioconductor.org/packages/release/bioc/manuals/dada2/man/dada2.pdf) tutorial by Benjamin Callahan that can be found [here](https://benjjneb.github.io/dada2/tutorial.html). 


## Inspecting the data {.tabset}

```{r set path to 16S samples}

## the directory containing the fastq files after unzipping.
path_16S <- "/projectnb/davieslab/bove/acer_MP/final_samples/16S_29Jun22" 

# looks good! need to specify only using the microplastic samples
# list.files(path_16S)

```

```{r extract fastq files and sample names}

# Forward and reverse fastq filenames have format: SAMPLENAME_R1.fastq and SAMPLENAME_R2.fastq
fnFs <- sort(list.files(path_16S, pattern = "_R1.fastq", full.names = TRUE))
fnRs <- sort(list.files(path_16S, pattern = "_R2.fastq", full.names = TRUE))

# Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq
sample_names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)

```

### Check samples to use

```{r select only microplastic samples}

## read in the meta data from the samples
meta <- read.csv("Data/Acerv_MP_Metadata.csv")


# filter the sample name list to select neg controls and microplastic samples only
sample_names <- sample_names[sample_names %in% meta$sampleID]
sample_names


# filter the forward sequences for microplastics only
fnFs <- fnFs %>% 
  str_detect("/A|/B|/C|G5|G6", negate = FALSE) %>%
  keep(fnFs, .)

# filter the reverse sequences for microplastics only
fnRs <- fnRs %>% 
  str_detect("/A|/B|/C|G5|G6", negate = FALSE) %>%
  keep(fnRs, .)

```

<br/>
<br/>


### View example quality plots

Viewing the quality of 2 of the forward samples
```{r firward quality plots}

## This first sample is giving us the error:
# Error: BiocParallel errors
#   1 remote errors, element index: 1
#   0 unevaluated and other errors
#   first remote error: 'x' contains missing values

## To fix this, I am going to go back and add '--minimum-length 1' to the 
# cutadapt step to discard processed reads that are shorter than LENGTH.

## Plot 2 samples for examples:
plotQualityProfile(fnFs[2:3])


## Saving the quality profiles of all (except sample A1) to view later if necessary
Fs_qualityPlot <- plotQualityProfile(fnFs[-1])

pdf(file = "Figures/Forward_qualityPlots.pdf", width = 18, height = 14)
Fs_qualityPlot
dev.off()

```


Viewing the quality of the same 2 of the reverse samples
```{r reverse quality plots}

## Plot 2 reverse samples for examples:
plotQualityProfile(fnRs[2:3])

## Saving the quality profiles of all to view later if necessary
Rs_qualityPlot <- plotQualityProfile(fnRs)

pdf(file = "Figures/Reverse_qualityPlots.pdf", width = 18, height = 14)
Rs_qualityPlot
dev.off()

```

<br/>
<br/>


## Filter and trim samples {.tabset}

### Perform the filtering

```{r}

# Place filtered files in filtered/ subdirectory
filtFs <- file.path(path_16S, "filtered", paste0(sample_names, "_filt_F.fastq.gz"))
filtRs <- file.path(path_16S, "filtered", paste0(sample_names, "_filt_R.fastq.gz"))

names(filtFs) <- sample_names
names(filtRs) <- sample_names

```

```{r}

out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs,
                     truncLen = c(220, 210), # cut for the forward and reverse files
                     maxN = 0, # DADA2 requires no Ns
                     maxEE = c(1, 1), # reads with higher than maxEE "expected errors" will be discarded
                     truncQ = 2, # Truncate reads at the first instance of a quality score less than or equal to truncQ
                     rm.phix = TRUE, # If TRUE, discard reads that match against the phiX genome (kinda control bacteria)
                     compress = TRUE, #  Whether the output fastq file should be gzip com- pressed.
                     multithread = TRUE) # On Windows set multithread = FALSE
head(out)

#the percent of reads making it through the filter and trim step:
colSums(out)[2]/colSums(out)[1] # 95.5%

```


<br/>
<br/>

### Check the filtering

Viewing the quality of the same 2 of the forward samples post filtering and trimming
```{r forward quality plots post filter trim}

## plot the filtered forward samples
plotQualityProfile(filtFs[2:3])


```

```{r save forward quality plots post filter trim, eval=FALSE, include=FALSE}

## save the filtered forward quality plots
pdf(file = "Figures/Forward_filt_qualityPlots.pdf", width = 18, height = 14)
plotQualityProfile(filtFs)
dev.off()

```

Viewing the quality of the same 2 of the reverse samples post filtering and trimming
```{r reverse quality plots post filter trim}

## plot the filtered reverse samples
plotQualityProfile(filtRs[2:3])

```

```{r save reverse quality plots post filter trim, eval=FALSE, include=FALSE}

## save the filtered reverse quality plots
pdf(file = "Figures/Reverse_filt_qualityPlots.pdf", width = 18, height = 14)
plotQualityProfile(filtRs)
dev.off()

```

<br/>
<br/>


### Error Rates

```{r}

errF <- learnErrors(filtFs, multithread = TRUE)
plotErrors(errF, nominalQ = TRUE)


errR <- learnErrors(filtRs, multithread = TRUE)
plotErrors(errR, nominalQ = TRUE)


dadaFs <- dada(filtFs, err=errF, multithread=TRUE)
dadaFs[[1]]

dadaRs <- dada(filtRs, err=errR, multithread=TRUE)
dadaRs[[1]]

```

<br/>
<br/>


### Sample Inference

We are now ready to apply [the core sample inference algorithm](https://www.nature.com/articles/nmeth.3869#methods) to the filtered and trimmed sequence data.

```{r}

dadaFs <- dada(filtFs, err = errF, multithread = TRUE)
dadaRs <- dada(filtRs, err = errR, multithread = TRUE)


# Inspecting the returned dada-class object:
dadaFs[[1]]

```

The DADA2 algorithm inferred 142 true sequence variants from the 9176 unique sequences in the first sample. There is much more to the dada-class return object than this (see help(`"dada-class"`) for some info), including multiple diagnostics about the quality of each denoised sequence variant, but that is beyond the scope of an introductory tutorial.


### Merge paired reads

We now merge the forward and reverse reads together to obtain the full denoised sequences. Merging is performed by aligning the denoised forward reads with the reverse-complement of the corresponding denoised reverse reads, and then constructing the merged “contig” sequences. By default, merged sequences are only output if the forward and reverse reads overlap by at least 12 bases, and are identical to each other in the overlap region (but these conditions can be changed via function arguments).

```{r}

mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose = TRUE)

# Inspect the merger data.frame from the first sample
head(mergers[[1]])

```

The mergers object is a list of data.frames from each sample. Each data.frame contains the merged sequence, its abundance, and the indices of the forward and reverse sequence variants that were merged. Paired reads that did not exactly overlap were removed by mergePairs, further reducing spurious output.

### Construct sequence table

We can now construct an amplicon sequence variant table (ASV) table, a higher-resolution version of the OTU table produced by traditional methods.

```{r}

seqtab <- makeSequenceTable(mergers)
dim(seqtab)

# Inspect distribution of read lengths
table(nchar(getSequences(seqtab)))
hist(nchar(getSequences(seqtab)))

# Remove any ASVs that are considerably off target length
seqtab_trimmed <- seqtab[,nchar(colnames(seqtab)) %in% seq(250,255)]
table(nchar(getSequences(seqtab_trimmed)))

```

The sequence table is a matrix with rows corresponding to (and named by) the samples, and columns corresponding to (and named by) the sequence variants. This table contains `r dim(seqtab)[2]` ASVs.

### Remove chimeras

The core dada method corrects substitution and indel errors, but chimeras remain. Fortunately, the accuracy of sequence variants after denoising makes identifying chimeric ASVs simpler than when dealing with fuzzy OTUs. Chimeric sequences are identified if they can be exactly reconstructed by combining a left-segment and a right-segment from two more abundant “parent” sequences.

```{r}

seqtab_nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab_nochim)

sum(seqtab_nochim)/sum(seqtab)


```


### Track reads through the pipeline

As a final check of our progress, we’ll look at the number of reads that made it through each step in the pipeline:

```{r}

getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab_nochim))
# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)

colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample_names
track

```

Looks good! We kept the majority of our raw reads, and there is no over-large drop associated with any single step.


### Assign taxonomy

It is common at this point, especially in 16S/18S/ITS amplicon sequencing, to assign taxonomy to the sequence variants. The DADA2 package provides a native implementation of the naive Bayesian classifier method for this purpose. The `assignTaxonomy` function takes as input a set of sequences to be classified and a training set of reference sequences with known taxonomy, and outputs taxonomic assignments with at least `minBoot` bootstrap confidence.

We maintain formatted training fastas for the RDP training set, GreenGenes clustered at 97% identity, and the Silva reference database, and additional trainings fastas suitable for protists and certain specific environments have been contributed. For fungal taxonomy, the General Fasta release files from the UNITE ITS database can be used as is. To follow along, download the silva_nr99_v138.1_train_set.fa.gz file, and place it in the directory with the fastq files.

The dada2 package GitHub maintains the most updated versions of the [Silva databases.](https://benjjneb.github.io/dada2/training.html). The versions in this GitHub repository, used here, were last updated on March 10, 2021.


```{r}

## going to move these out of this repo because they are too big:
# path to SILVA files: /projectnb/davieslab/bove/SILVA_files


taxa <- assignTaxonomy(seqtab_nochim, "/projectnb/davieslab/bove/SILVA_files/silva_nr99_v138.1_train_set.fa.gz", multithread=TRUE)

# Extensions: The dada2 package also implements a method to make species level assignments based on exact matching between
# ASVs and sequenced reference strains. Recent analysis suggests that exact matching (or 100% identity) is the only
# appropriate way to assign species to 16S gene fragments. Currently, species-assignment training fastas are available for
# the Silva and RDP 16S databases. To follow the optional species addition step, download the
# silva_species_assignment_v138.1.fa.gz file, and place it in the directory with the fastq files.

taxa_sp <- addSpecies(taxa, "/projectnb/davieslab/bove/SILVA_files/silva_species_assignment_v138.1.fa.gz")

```

Let’s inspect the taxonomic assignments:

```{r}

taxa_print <- taxa_sp # Removing sequence rownames for display only
rownames(taxa_print) <- NULL
head(taxa_print)

```

**Considerations for your own data:** If your reads do not seem to be appropriately assigned, for example lots of your bacterial 16S sequences are being assigned as Eukaryota NA NA NA NA NA, your reads may be in the opposite orientation as the reference database. Tell dada2 to try the reverse-complement orientation with assignTaxonomy(..., tryRC=TRUE) and see if this fixes the assignments. If using DECIPHER for taxonomy, try IdTaxa (..., strand="both").


### Handoff to phyloseq

```{r}

## Install missing packages
# install.packages("Biostrings")
# 
# if (!require("BiocManager", quietly = TRUE))
#     install.packages("BiocManager")
# BiocManager::install(version = "3.14")
# BiocManager::install("phyloseq")

library(phyloseq)
library(Biostrings)
library(ggplot2)

theme_set(theme_bw())

```

```{r}

# Export sequence table with genus and species assignments as phyloseq objects
pylo <- phyloseq(otu_table(seqtab_nochim, taxa_are_rows=FALSE), tax_table(taxa))
pylo_sp <- phyloseq(otu_table(seqtab_nochim, taxa_are_rows=FALSE), tax_table(taxa_sp))

#originally, sequences were the taxa names
seqs_ps <- taxa_names(pylo_sp)

#change taxa names to ASVs
taxa_names(pylo_sp) <- paste0("ASV", seq(ntaxa(pylo_sp)))

# save a file of the taxa table with ASVs and sequences
taxtab_seqs <- cbind(data.frame(tax_table(pylo_sp)), seqs_ps)
write.csv(taxtab_seqs,"Data/taxatable_withsequences.csv")


# Save as RDS objects
saveRDS(pylo, file = "Data/phylo.rds") # does not have species level
saveRDS(pylo_sp, file = "Data/pylo_sp.rds") # with species level and ASV labeling

```

## Phyloseq pipeline

We can construct a simple sample data.frame from the information encoded in the filenames. Usually this step would instead involve reading the sample data in from a file.

```{r}

head(meta)

row.names(meta) <- meta$sampleID

```

We now construct a phyloseq object directly from the dada2 outputs.

```{r}

ps <- phyloseq(otu_table(seqtab_nochim, taxa_are_rows=FALSE), 
               sample_data(data.frame(meta[c(3,4)])), 
               tax_table(taxa))


dna <- Biostrings::DNAStringSet(taxa_names(ps))
names(dna) <- taxa_names(ps)
ps <- merge_phyloseq(ps, dna)
taxa_names(ps) <- paste0("ASV", seq(ntaxa(ps)))
ps



# We are now ready to use phyloseq!

## Visualize alpha-diversity:

plot_richness(ps, x = "Treatment", measures = c("Shannon", "Simpson"), color = "Genotype")

## WARNING:
# Warning message:
# In estimate_richness(physeq, split = TRUE, measures = measures) :
#   The data you have provided does not have
# any singletons. This is highly suspicious. Results of richness
# estimates (for example) are probably unreliable, or wrong, if you have already
# trimmed low-abundance taxa from the data.
# 
# We recommended that you find the un-trimmed data and retry.



## Ordinate:

# Transform data to proportions as appropriate for Bray-Curtis distances
ps.prop <- transform_sample_counts(ps, function(otu) otu/sum(otu))
ord.nmds.bray <- ordinate(ps.prop, method="NMDS", distance="bray")


plot_ordination(ps.prop, ord.nmds.bray, color="Treatment", title="Bray NMDS")


## Bar plot:

top20 <- names(sort(taxa_sums(ps), decreasing=TRUE))[1:20]
ps.top20 <- transform_sample_counts(ps, function(OTU) OTU/sum(OTU))
ps.top20 <- prune_taxa(top20, ps.top20)
# plot_bar(ps.top20, x="Treatment", fill="Class") #+ facet_wrap(~When, scales="free_x")

```

```{r}

plot_bar2 <- function (physeq, x = "Sample", y = "Abundance", fill = NULL, colour = NULL,
  title = NULL, facet_grid = NULL) 
{
  mdf = psmelt(physeq)
  p = ggplot(mdf, aes_string(x = x, y = y, fill = fill))
  p = p + geom_bar(stat = "identity", position = "stack")
  p = p + theme(axis.text.x = element_text(angle = -90, hjust = 0))
  if (!is.null(facet_grid)) {
    p <- p + facet_grid(facet_grid)
  }
  if (!is.null(title)) {
    p <- p + ggtitle(title)
  }
  return(p)
}


king_plot <- plot_bar2(ps.top20, x="Treatment", fill = "Kingdom", colour = "Family")
phy_plot <- plot_bar2(ps.top20, x="Treatment", fill = "Phylum", colour = "Family")
clas_plot <- plot_bar2(ps.top20, x="Treatment", fill = "Class", colour = "Family")
order_plot <- plot_bar2(ps.top20, x="Treatment", fill = "Order", colour = "Family")
fam_plot <- plot_bar2(ps.top20, x="Treatment", fill = "Family", colour = "Family")
gen_plot <- plot_bar2(ps.top20, x="Treatment", fill = "Genus", colour = "Genus")

library(ggpubr)
ggarrange(king_plot, phy_plot, clas_plot, order_plot, fam_plot, gen_plot, ncol = 2, nrow = 3)

```



```{r eval=FALSE, include=FALSE}

library("dada2")
library("seqinr")
library("biomformat")
library("data.table")
library("ggplot2")
# library("ggthemes")
# library("ampvis2")
library("cowplot")
library("phyloseq")


# phyloseq object output from Dada2
ps <- readRDS("Data/phylo.rds")
seqtab <- readRDS("Data/pylo_sp.rds")
# sequences object made from Dada2

# exporting sequences to a fasta file for import into qiime
uniqueSeqs <- as.list(colnames(seqtab))
write.fasta(uniqueSeqs, uniqueSeqs, "uniqueSeqs.fasta")

# import metadata and merge into phyloseq object
mapfile = "Acer_microbiome_metadata_NR.txt"
map = import_qiime_sample_data(mapfile)
sample_data(ps) <- map

# export .tsv asv_table to import into qiime2
otu<-t(as(otu_table(ps),"matrix"))
otu_biom<-make_biom(data=otu)
write_biom(otu_biom,"ps.biom")

# export taxonomy to import into qiime2
tax<-as(tax_table(ps),"matrix")
tax_cols <- c("Kingdom", "Phylum", "Class", "Order","Family","Genus", "Species")
tax<-as.data.frame(tax)
tax$taxonomy<-do.call(paste, c(tax[tax_cols], sep=";"))
for(co in tax_cols) tax[co]<-NULL
write.table(tax, "taxonomy.txt", quote=FALSE, col.names=FALSE, sep="\t")

# summary of data
ps
summary(sample_data(ps))

ntaxa(ps)
nsamples(ps)
rank_names(ps)
sample_names(ps)[1:5]
sample_variables(ps)

# remove mitochondria and chloroplasts, is.na important becuase if not included
# this command will also remove all Family = NA or Order = NA
ps_with_mito = subset_taxa(ps, (Order!="Chloroplast") | is.na(Order))
ps_no_mito = subset_taxa(ps_with_mito, (Family!="Mitochondria") | is.na(Family))
ps_no_Eukaryota = subset_taxa(ps_no_mito, (Kingdom!="Eukaryota") | is.na(Kingdom))

ps = ps_no_Eukaryota #lost 71 taxa

#sara's pruning step
pstax_filt<-filter_taxa(ps,function(x) sum(x>10) > (0.01*length(x)),TRUE)

summary_tab <- data.frame(init=(as.matrix(sample_sums(ps_full)))[,1],
                           chloros_removed=(as.matrix(sample_sums(ps_with_mito)))[,1],
                           mitos_removed=(as.matrix(sample_sums(ps_no_mito)))[,1],
                           euks_removed=(as.matrix(sample_sums(ps_no_Eukaryota)))[,1],
                           pruned=(as.matrix(sample_sums(pstax_filt)))[,1])
write.table(summary_tab, "reads_lost_phyloseq.txt", quote=FALSE, col.names=FALSE, sep="\t")

# pst = fast_melt(ps)
# prevdt = pst[, list(Prevalence = sum(count > 0), 
#                     TotalCounts = sum(count)),
#              by = taxaID]
# keepTaxa = prevdt[(Prevalence >=0 & TotalCounts >15), taxaID]
# ps_pruned = prune_taxa(keepTaxa,ps)
# ps_pruned
# sample_sums(ps_pruned)
# min(sample_sums(ps_pruned))
# ps <- ps_pruned
# save(ps, file = "ps_pruned.RData")

#plot otus by sequencing depth
observed <- estimate_richness(ps, measures = c('Observed'))
explore.df <- cbind(observed, sample_sums(ps), sample_data(ps)$Location)
colnames(explore.df) <- c('Observed', 'Sample_Sums', 'Location')
observed_mean <- mean(explore.df$Observed)
sample_sum_mean <- mean(explore.df$Sample_Sums)
ggplot(data = explore.df, aes(x = Sample_Sums, y = Observed, color = Location)) + 
  geom_point() +
  geom_smooth(method="auto", se=TRUE, fullrange=FALSE, level=0.95, 
              inherit.aes = F, mapping = aes(Sample_Sums, Observed),
              data = explore.df) +
  ylab("Observed OTUs") +
  scale_colour_colorblind()

# Let's use ampvis2 again so we can easily make a rarefaction curve

# Need to convert from phyloseq to ampvis
av2_otutable <- data.frame(OTU = rownames(t(phyloseq::otu_table(ps)@.Data)),
                           t(phyloseq::otu_table(ps)@.Data),
                           phyloseq::tax_table(ps)@.Data,
                           check.names = F
)

#Extract metadata from the phyloseq object:
av2_metadata <- data.frame(phyloseq::sample_data(ps), 
                           check.names = F
)

av2_metadata <- cbind(rownames(av2_metadata), av2_metadata)

#Load the data with amp_load:
av2_obj <- amp_load(av2_otutable, av2_metadata)

# RARE CURVE
rare_plot_amp <- amp_rarecurve(data = av2_obj, color_by = "Region")
rare_curve_plot <- rare_plot_amp + ylab('Observed ASVs (count)') + 
  geom_vline(xintercept=min(sample_sums(ps)), linetype='dashed') +
  scale_colour_colorblind() +
  xlim(c(0, 35000))
plot(rare_plot_amp)
rare_curve_plot


#If we rarefy to 26537
rare_plot_amp <- amp_rarecurve(data = av2_obj, color_by = "Location")
rare_curve_plot <- rare_plot_amp + ylab('Observed ASVs (count)') + 
  geom_vline(xintercept=c(26537), linetype='dashed') +
  scale_colour_colorblind() +
  xlim(c(0, 35000))
#plot(rare_plot_amp)
rare_curve_plot

summary(explore.df$Sample_Sums)
length(explore.df$Sample_Sums)

# plot alpha diversity
plot_richness(ps, x = "Location", color = "Geno", measures = c('Shannon', 'Simpson'))

ps_rarefied <- rarefy_even_depth(ps, sample.size = 26537, rngseed = 999) #546 OTUs removed
ps_rarefied
sum(sample_sums(ps_rarefied))
sample_sums(ps_rarefied)

# # plot alpha diversity
richness.rare <- cbind(estimate_richness(ps_rarefied, 
                                          measures = c('Shannon', 'Simpson')),
                        sample_data(ps_rarefied)$Location)
colnames(richness.rare) <- c('Shannon', 'Simpson', 'Location')
richness.rare$Labels <- rownames(richness.rare)
# 
ggplot(data = richness.rare, aes(x = Shannon, y = Simpson, color = Location)) + 
   geom_point()

ggplot(data = richness.rare, aes(x = Shannon, y = (Simpson-1)*-1, color = Location)) + 
   geom_point() +
     geom_smooth(method="auto", se=TRUE, fullrange=FALSE, level=0.95, 
              inherit.aes = F, mapping = aes(Shannon, Simpson),
                 data = richness.rare) +
   scale_color_colorblind() +
   geom_text(aes(label=ifelse(Shannon<1, Labels, ""), hjust=-0.1),
             show.legend = F) +
   xlim(c(0,max(richness.rare$Shannon))) +
   theme_cowplot()

#relative abundance transform
ps_rel = filter_taxa(ps_rarefied, function(x) mean(x) > 0.1, TRUE)
ps_rel = transform_sample_counts(ps_rel, function(x) x / sum(x) )
ps_rel



# save ps object
save(ps, file = "ps_unrarefied.RData")
save(ps_rarefied, file = "ps_rare_acer.RData")
save(ps_rel, file = "ps_rel.RData")

```


## Session Information

All code was written by [Colleen B. Bove](https://colleenbove.science), feel free to contact with questions.

Session information from the last run date on `r format(Sys.time(), '%d %B %Y')`:

```{r print session info}

sessionInfo()

```
